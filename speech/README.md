In this work, we extracted features from raw waveform using wav2vec2.0. This model takes raw waveform as input and then applies two networks: the encoder network and the context network. Multi-layer convolution feature encoder f∶ X→Z takes as input raw signal and outputs latent speech representations z_1,…,z_T for T-time steps. The result obtained lends itself to the input of the Transformer g∶ Z→C to build representations c_1,…,c_T capturing information from the entire sequence. The output of the feature encoder is discretized to q_t with a quantization module Z→Q.
We use the large module for feature extraction. The Large module contains 24 transformer blocks with model dimension 1024, inner dimension 4096, and 16 attention heads. We assume that the representation contains not only important information for speech recognition, but also for emotion recognition.
