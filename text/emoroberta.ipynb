{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe04358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Adoptation for the model proposed here: https://skimai.com/fine-tuning-bert-for-sentiment-analysis/\n",
    "- goemotions dataset support\n",
    "- emotions mapping to the Plutchik's scale\n",
    "- turned to multilabel classification with neutral class\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "    \n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "#from  ipynb.fs.full.testing_func import calc_classes_confusion, plot_classes_confusion, print_metrics, pr_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077b9b1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('goemotions_3.csv')\n",
    "data.dropna(subset = ['text'], inplace = True)\n",
    "class_names = data.columns[3:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b3ea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb3559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e614b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[:, ['text','anger',  'fear', 'joy',   'sadness','surprise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c8447d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Worst ending ever! I won't spoil it but this o...\n",
       "1                       Happy cake day u/sneakpeekbot!\n",
       "2    Was he rejected because of his methodology or ...\n",
       "3                                      thanks, I agree\n",
       "4                      Why would you doubt it dumbass?\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cfd2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.iloc[:, 1:].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "707b6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85409667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(np.where(labels == 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f37b69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.iloc[:,1:].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545e59af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why would you doubt it dumbass?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[NAME] is a hungry little bastard.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Both parents were drunk. Where were the small ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OMG! I can only imagine. I've gotten it into a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ONLY FUCKING MERRILL WOULD MISS THAT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  anger  fear  joy  \\\n",
       "4                     Why would you doubt it dumbass?      1     0    0   \n",
       "6                  [NAME] is a hungry little bastard.      1     0    0   \n",
       "9   Both parents were drunk. Where were the small ...      0     0    0   \n",
       "18  OMG! I can only imagine. I've gotten it into a...      0     0    0   \n",
       "29               ONLY FUCKING MERRILL WOULD MISS THAT      1     0    0   \n",
       "\n",
       "    sadness  surprise  \n",
       "4         0         0  \n",
       "6         0         0  \n",
       "9         1         0  \n",
       "18        0         1  \n",
       "29        0         0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32caf1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
    "\n",
    "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
    "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n",
    "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n",
    "\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
    "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
    "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
    "                'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "267c2437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Артур\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Артур\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#!pip install contractions\n",
    "import contractions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "def preprocess(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #print(sentence)\n",
    "    #sentence =sentence.tolist()\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sentence = re.sub('[^A-z]', ' ', sentence)\n",
    "    negative = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n",
    "                        'even though', 'yet']\n",
    "    stop_words = [z for z in stop_words if z not in negative]\n",
    "    preprocessed_tokens = [lemmatizer.lemmatize(contractions.fix(temp.lower())) for temp in sentence.split() if temp not in stop_words] #lemmatization\n",
    "    return ' '.join([x for x in preprocessed_tokens]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e84e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from bs4 import BeautifulSoup\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel, AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "def clean_text(text):\n",
    "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r'\\:(.*?)\\:','',text)\n",
    "    text = str(text).lower()    #Making Text Lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #The next 2 lines remove html text\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    '''Clean contraction using contraction mapping'''    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    for word in mapping.keys():\n",
    "        if \"\"+word+\"\" in text:\n",
    "            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n",
    "    #Remove Punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    '''Cleans special characters present(if any)'''   \n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    '''Corrects common spelling errors'''   \n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def remove_space(text):\n",
    "    '''Removes awkward spaces'''   \n",
    "    #Removes awkward spaces \n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return \" \".join(text)\n",
    "\n",
    "def text_preprocessing_pipeline(text):\n",
    "    '''Cleaning and parsing the text.'''\n",
    "    text = clean_text(text)\n",
    "    text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_special_chars(text, punct, punct_mapping)\n",
    "    text = correct_spelling(text, mispell_dict)\n",
    "    text = remove_space(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63c81848",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.text.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "845054b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6309 in train, and 2104 in test 2104 in val\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(texts, \n",
    "    labels, \n",
    "    test_size = 0.2,\n",
    "    shuffle = True)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "    test_size=0.25, random_state= 8) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "print(len(x_train), 'in train, and', len(x_test), 'in test', len(x_val), 'in val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dec56e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['anger',  'fear', 'joy', 'sadness', 'surprise']\n",
    "num_labels = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e91ce602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "tokenizer = RobertaTokenizer.from_pretrained(path)\n",
    "#model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88b3579b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  65\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "encoded_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(text) for text in encoded_texts])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20a99245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for text in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,             # Max length to truncate/pad\n",
    "            truncation=True,\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True# Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        token_type_ids.append(encoded_sent.get('token_type_ids'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    token_type_ids = torch.tensor(token_type_ids)\n",
    "    return input_ids, attention_masks, token_type_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b69534ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Why would you doubt it dumbass?\n",
      "Token IDs:  [0, 7608, 74, 47, 2980, 24, 16881, 2401, 116, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Specify MAX_LEN`\n",
    "MAX_LEN = 65\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = list(preprocessing_for_bert([texts[0]])[0].squeeze().numpy())\n",
    "print('Original: ', texts[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks, train_type_ids = preprocessing_for_bert(x_train)\n",
    "test_inputs, test_masks, test_type_ids = preprocessing_for_bert(x_test)\n",
    "val_inputs, val_masks, val_type_ids = preprocessing_for_bert(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "210b03fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "from transformers.models.roberta.modeling_roberta import (\n",
    "    RobertaPreTrainedModel,\n",
    "    RobertaModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bcb05c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_labels = torch.tensor(y_test)\n",
    "val_labels = torch.tensor(y_val)\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_type_ids, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_type_ids, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_type_ids, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e24f7be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    # Bert Model for Classification Tasks.\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 256, 5\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert =RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7543e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=1e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0, # Default value\n",
    "        num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b5a136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_weight = torch.tensor(11.634)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa5f0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Specify loss function\n",
    "#loss_fn_train = torch.nn.BCEWithLogitsLoss(torch.tensor((y_train==0.).sum()/y_train.sum()))\n",
    "#loss_fn_val = torch.nn.BCEWithLogitsLoss(torch.tensor((y_val==0.).sum()/y_val.sum()))\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "#loss_fn = BCEWithLogitsLossWeighted(100)\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    train_history = []\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_token, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask, b_token)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            \n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            train_history.append(loss.cpu().detach().numpy())\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return train_history\n",
    "\n",
    "def evaluate(model, val_dataloader, treshold=0.5):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask,b_token, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask, b_token)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = logits >= treshold\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "      #  print(logits)\n",
    "       # accuracy = np.mean([(preds[:, i] == b_labels[:, i]).cpu().numpy().mean() for i in range(logits.shape[1])])\n",
    "        result = multi_label_metrics(logits, b_labels, treshold)\n",
    "        val_accuracy.append(result['accuracy'])\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d4f0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    #predictions = torch.tensor(predictions)\n",
    "   # labels = torch.tensor(labels)\n",
    "  #  predictions = predictions.cpu()\n",
    "    #labels = labels.cpu()\n",
    "   # print(predictions)\n",
    "   # predictions = torch.stack(predictions, axis = 1)\n",
    "    probs = sigmoid(predictions).cpu()\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    \n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = np.zeros(probs.shape)\n",
    "    #print('predictions:',predictions, 'labels:', labels, 'probs', probs)\n",
    "    labels = torch.tensor(labels.cpu())\n",
    "    y_true[np.where(labels >= threshold)] = 1\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "   # print(predictions.shape, y_true.shape)\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy,\n",
    "               'prediction':y_pred\n",
    "              }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c04d8040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.638243   |     -      |     -     |   5.58   \n",
      "   1    |   40    |   0.518661   |     -      |     -     |   2.71   \n",
      "   1    |   60    |   0.507541   |     -      |     -     |   2.72   \n",
      "   1    |   80    |   0.486719   |     -      |     -     |   2.72   \n",
      "   1    |   100   |   0.492513   |     -      |     -     |   2.72   \n",
      "   1    |   120   |   0.458874   |     -      |     -     |   2.72   \n",
      "   1    |   140   |   0.449157   |     -      |     -     |   2.72   \n",
      "   1    |   160   |   0.406707   |     -      |     -     |   2.72   \n",
      "   1    |   180   |   0.398171   |     -      |     -     |   2.72   \n",
      "   1    |   200   |   0.387086   |     -      |     -     |   2.73   \n",
      "   1    |   220   |   0.357099   |     -      |     -     |   2.72   \n",
      "   1    |   240   |   0.354019   |     -      |     -     |   2.72   \n",
      "   1    |   260   |   0.338707   |     -      |     -     |   2.73   \n",
      "   1    |   280   |   0.338102   |     -      |     -     |   2.73   \n",
      "   1    |   300   |   0.327007   |     -      |     -     |   2.73   \n",
      "   1    |   320   |   0.307430   |     -      |     -     |   2.73   \n",
      "   1    |   340   |   0.290913   |     -      |     -     |   2.73   \n",
      "   1    |   360   |   0.282019   |     -      |     -     |   2.73   \n",
      "   1    |   380   |   0.280859   |     -      |     -     |   2.73   \n",
      "   1    |   400   |   0.272901   |     -      |     -     |   2.73   \n",
      "   1    |   420   |   0.284119   |     -      |     -     |   2.74   \n",
      "   1    |   440   |   0.309333   |     -      |     -     |   2.73   \n",
      "   1    |   460   |   0.310816   |     -      |     -     |   2.73   \n",
      "   1    |   480   |   0.283819   |     -      |     -     |   2.73   \n",
      "   1    |   500   |   0.290206   |     -      |     -     |   2.74   \n",
      "   1    |   520   |   0.283756   |     -      |     -     |   2.73   \n",
      "   1    |   540   |   0.278400   |     -      |     -     |   2.73   \n",
      "   1    |   560   |   0.282494   |     -      |     -     |   2.73   \n",
      "   1    |   580   |   0.303002   |     -      |     -     |   2.74   \n",
      "   1    |   600   |   0.289767   |     -      |     -     |   2.74   \n",
      "   1    |   620   |   0.271768   |     -      |     -     |   2.74   \n",
      "   1    |   640   |   0.282277   |     -      |     -     |   2.74   \n",
      "   1    |   660   |   0.269069   |     -      |     -     |   2.74   \n",
      "   1    |   680   |   0.283195   |     -      |     -     |   2.74   \n",
      "   1    |   700   |   0.256524   |     -      |     -     |   2.74   \n",
      "   1    |   720   |   0.312587   |     -      |     -     |   2.74   \n",
      "   1    |   740   |   0.253591   |     -      |     -     |   2.74   \n",
      "   1    |   760   |   0.304427   |     -      |     -     |   2.74   \n",
      "   1    |   780   |   0.288573   |     -      |     -     |   2.74   \n",
      "   1    |   788   |   0.304637   |     -      |     -     |   1.15   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.341805   |  0.256297  |   0.68    |  117.94  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.223427   |     -      |     -     |   2.81   \n",
      "   2    |   40    |   0.252267   |     -      |     -     |   2.74   \n",
      "   2    |   60    |   0.185834   |     -      |     -     |   2.74   \n",
      "   2    |   80    |   0.255373   |     -      |     -     |   2.74   \n",
      "   2    |   100   |   0.231658   |     -      |     -     |   2.75   \n",
      "   2    |   120   |   0.224810   |     -      |     -     |   2.74   \n",
      "   2    |   140   |   0.221792   |     -      |     -     |   2.74   \n",
      "   2    |   160   |   0.212746   |     -      |     -     |   2.74   \n",
      "   2    |   180   |   0.221546   |     -      |     -     |   2.74   \n",
      "   2    |   200   |   0.213926   |     -      |     -     |   2.74   \n",
      "   2    |   220   |   0.230119   |     -      |     -     |   2.74   \n",
      "   2    |   240   |   0.233854   |     -      |     -     |   2.74   \n",
      "   2    |   260   |   0.245295   |     -      |     -     |   2.75   \n",
      "   2    |   280   |   0.274065   |     -      |     -     |   2.75   \n",
      "   2    |   300   |   0.208080   |     -      |     -     |   2.75   \n",
      "   2    |   320   |   0.215107   |     -      |     -     |   2.78   \n",
      "   2    |   340   |   0.244040   |     -      |     -     |   2.79   \n",
      "   2    |   360   |   0.241427   |     -      |     -     |   2.75   \n",
      "   2    |   380   |   0.217764   |     -      |     -     |   2.79   \n",
      "   2    |   400   |   0.261878   |     -      |     -     |   2.75   \n",
      "   2    |   420   |   0.271020   |     -      |     -     |   2.78   \n",
      "   2    |   440   |   0.260044   |     -      |     -     |   2.80   \n",
      "   2    |   460   |   0.269476   |     -      |     -     |   2.85   \n",
      "   2    |   480   |   0.230278   |     -      |     -     |   2.75   \n",
      "   2    |   500   |   0.204780   |     -      |     -     |   2.75   \n",
      "   2    |   520   |   0.244348   |     -      |     -     |   2.75   \n",
      "   2    |   540   |   0.232414   |     -      |     -     |   2.75   \n",
      "   2    |   560   |   0.235690   |     -      |     -     |   2.75   \n",
      "   2    |   580   |   0.201543   |     -      |     -     |   2.76   \n",
      "   2    |   600   |   0.247088   |     -      |     -     |   2.77   \n",
      "   2    |   620   |   0.204204   |     -      |     -     |   2.73   \n",
      "   2    |   640   |   0.226482   |     -      |     -     |   2.75   \n",
      "   2    |   660   |   0.275419   |     -      |     -     |   2.75   \n",
      "   2    |   680   |   0.191746   |     -      |     -     |   2.75   \n",
      "   2    |   700   |   0.216551   |     -      |     -     |   2.75   \n",
      "   2    |   720   |   0.232145   |     -      |     -     |   2.75   \n",
      "   2    |   740   |   0.281965   |     -      |     -     |   2.75   \n",
      "   2    |   760   |   0.183062   |     -      |     -     |   2.75   \n",
      "   2    |   780   |   0.202785   |     -      |     -     |   2.75   \n",
      "   2    |   788   |   0.219092   |     -      |     -     |   1.09   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.231302   |  0.252478  |   0.73    |  116.04  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.194834   |     -      |     -     |   2.84   \n",
      "   3    |   40    |   0.167765   |     -      |     -     |   2.78   \n",
      "   3    |   60    |   0.181086   |     -      |     -     |   2.74   \n",
      "   3    |   80    |   0.225068   |     -      |     -     |   2.75   \n",
      "   3    |   100   |   0.211160   |     -      |     -     |   2.75   \n",
      "   3    |   120   |   0.192803   |     -      |     -     |   2.75   \n",
      "   3    |   140   |   0.210988   |     -      |     -     |   2.76   \n",
      "   3    |   160   |   0.217435   |     -      |     -     |   2.75   \n",
      "   3    |   180   |   0.209879   |     -      |     -     |   2.74   \n",
      "   3    |   200   |   0.201406   |     -      |     -     |   2.74   \n",
      "   3    |   220   |   0.210706   |     -      |     -     |   2.75   \n",
      "   3    |   240   |   0.220244   |     -      |     -     |   2.75   \n",
      "   3    |   260   |   0.173303   |     -      |     -     |   2.75   \n",
      "   3    |   280   |   0.200392   |     -      |     -     |   2.74   \n",
      "   3    |   300   |   0.214180   |     -      |     -     |   2.75   \n",
      "   3    |   320   |   0.178534   |     -      |     -     |   2.75   \n",
      "   3    |   340   |   0.194707   |     -      |     -     |   2.75   \n",
      "   3    |   360   |   0.145384   |     -      |     -     |   2.76   \n",
      "   3    |   380   |   0.201776   |     -      |     -     |   2.75   \n",
      "   3    |   400   |   0.156635   |     -      |     -     |   2.74   \n",
      "   3    |   420   |   0.165468   |     -      |     -     |   2.75   \n",
      "   3    |   440   |   0.209063   |     -      |     -     |   2.75   \n",
      "   3    |   460   |   0.181109   |     -      |     -     |   2.74   \n",
      "   3    |   480   |   0.219361   |     -      |     -     |   2.75   \n",
      "   3    |   500   |   0.190740   |     -      |     -     |   2.75   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3    |   520   |   0.163770   |     -      |     -     |   2.75   \n",
      "   3    |   540   |   0.208595   |     -      |     -     |   2.75   \n",
      "   3    |   560   |   0.176208   |     -      |     -     |   2.75   \n",
      "   3    |   580   |   0.167466   |     -      |     -     |   2.75   \n",
      "   3    |   600   |   0.219015   |     -      |     -     |   2.75   \n",
      "   3    |   620   |   0.197982   |     -      |     -     |   2.75   \n",
      "   3    |   640   |   0.179566   |     -      |     -     |   2.75   \n",
      "   3    |   660   |   0.193254   |     -      |     -     |   2.75   \n",
      "   3    |   680   |   0.232038   |     -      |     -     |   2.75   \n",
      "   3    |   700   |   0.225796   |     -      |     -     |   2.76   \n",
      "   3    |   720   |   0.203071   |     -      |     -     |   2.75   \n",
      "   3    |   740   |   0.216647   |     -      |     -     |   2.76   \n",
      "   3    |   760   |   0.210869   |     -      |     -     |   2.74   \n",
      "   3    |   780   |   0.219979   |     -      |     -     |   2.77   \n",
      "   3    |   788   |   0.139242   |     -      |     -     |   1.10   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.196546   |  0.261317  |   0.74    |  115.96  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   20    |   0.176797   |     -      |     -     |   2.83   \n",
      "   4    |   40    |   0.161432   |     -      |     -     |   2.76   \n",
      "   4    |   60    |   0.165013   |     -      |     -     |   2.75   \n",
      "   4    |   80    |   0.180688   |     -      |     -     |   2.76   \n",
      "   4    |   100   |   0.182471   |     -      |     -     |   2.75   \n",
      "   4    |   120   |   0.152455   |     -      |     -     |   2.75   \n",
      "   4    |   140   |   0.172649   |     -      |     -     |   2.75   \n",
      "   4    |   160   |   0.185206   |     -      |     -     |   2.75   \n",
      "   4    |   180   |   0.137767   |     -      |     -     |   2.76   \n",
      "   4    |   200   |   0.155807   |     -      |     -     |   2.75   \n",
      "   4    |   220   |   0.127184   |     -      |     -     |   2.75   \n",
      "   4    |   240   |   0.148626   |     -      |     -     |   2.76   \n",
      "   4    |   260   |   0.170060   |     -      |     -     |   2.75   \n",
      "   4    |   280   |   0.181067   |     -      |     -     |   2.75   \n",
      "   4    |   300   |   0.123842   |     -      |     -     |   2.76   \n",
      "   4    |   320   |   0.196100   |     -      |     -     |   2.76   \n",
      "   4    |   340   |   0.152131   |     -      |     -     |   2.78   \n",
      "   4    |   360   |   0.155831   |     -      |     -     |   2.75   \n",
      "   4    |   380   |   0.224878   |     -      |     -     |   2.76   \n",
      "   4    |   400   |   0.143696   |     -      |     -     |   2.76   \n",
      "   4    |   420   |   0.130607   |     -      |     -     |   2.75   \n",
      "   4    |   440   |   0.204835   |     -      |     -     |   2.75   \n",
      "   4    |   460   |   0.140246   |     -      |     -     |   2.77   \n",
      "   4    |   480   |   0.164082   |     -      |     -     |   2.76   \n",
      "   4    |   500   |   0.161141   |     -      |     -     |   2.76   \n",
      "   4    |   520   |   0.117342   |     -      |     -     |   2.75   \n",
      "   4    |   540   |   0.183163   |     -      |     -     |   2.76   \n",
      "   4    |   560   |   0.185109   |     -      |     -     |   2.78   \n",
      "   4    |   580   |   0.221249   |     -      |     -     |   2.75   \n",
      "   4    |   600   |   0.167507   |     -      |     -     |   2.75   \n",
      "   4    |   620   |   0.160831   |     -      |     -     |   2.76   \n",
      "   4    |   640   |   0.161108   |     -      |     -     |   2.75   \n",
      "   4    |   660   |   0.185356   |     -      |     -     |   2.75   \n",
      "   4    |   680   |   0.151799   |     -      |     -     |   2.75   \n",
      "   4    |   700   |   0.159807   |     -      |     -     |   2.76   \n",
      "   4    |   720   |   0.164017   |     -      |     -     |   2.75   \n",
      "   4    |   740   |   0.170639   |     -      |     -     |   2.74   \n",
      "   4    |   760   |   0.175370   |     -      |     -     |   2.75   \n",
      "   4    |   780   |   0.143364   |     -      |     -     |   2.75   \n",
      "   4    |   788   |   0.255848   |     -      |     -     |   1.09   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.166095   |  0.271874  |   0.74    |  116.19  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   5    |   20    |   0.141370   |     -      |     -     |   2.83   \n",
      "   5    |   40    |   0.139069   |     -      |     -     |   2.78   \n",
      "   5    |   60    |   0.150658   |     -      |     -     |   2.79   \n",
      "   5    |   80    |   0.143685   |     -      |     -     |   2.74   \n",
      "   5    |   100   |   0.115577   |     -      |     -     |   2.79   \n",
      "   5    |   120   |   0.172146   |     -      |     -     |   2.75   \n",
      "   5    |   140   |   0.191319   |     -      |     -     |   2.75   \n",
      "   5    |   160   |   0.129092   |     -      |     -     |   2.76   \n",
      "   5    |   180   |   0.162697   |     -      |     -     |   2.75   \n",
      "   5    |   200   |   0.154141   |     -      |     -     |   2.76   \n",
      "   5    |   220   |   0.156397   |     -      |     -     |   2.75   \n",
      "   5    |   240   |   0.193790   |     -      |     -     |   2.75   \n",
      "   5    |   260   |   0.144601   |     -      |     -     |   2.76   \n",
      "   5    |   280   |   0.121424   |     -      |     -     |   2.76   \n",
      "   5    |   300   |   0.165846   |     -      |     -     |   2.75   \n",
      "   5    |   320   |   0.127311   |     -      |     -     |   2.75   \n",
      "   5    |   340   |   0.168919   |     -      |     -     |   2.75   \n",
      "   5    |   360   |   0.138120   |     -      |     -     |   2.76   \n",
      "   5    |   380   |   0.103236   |     -      |     -     |   2.75   \n",
      "   5    |   400   |   0.117125   |     -      |     -     |   2.76   \n",
      "   5    |   420   |   0.175777   |     -      |     -     |   2.76   \n",
      "   5    |   440   |   0.152428   |     -      |     -     |   2.76   \n",
      "   5    |   460   |   0.148438   |     -      |     -     |   2.77   \n",
      "   5    |   480   |   0.155400   |     -      |     -     |   2.76   \n",
      "   5    |   500   |   0.140593   |     -      |     -     |   2.77   \n",
      "   5    |   520   |   0.158815   |     -      |     -     |   2.79   \n",
      "   5    |   540   |   0.153950   |     -      |     -     |   2.73   \n",
      "   5    |   560   |   0.123469   |     -      |     -     |   2.77   \n",
      "   5    |   580   |   0.118271   |     -      |     -     |   2.75   \n",
      "   5    |   600   |   0.169560   |     -      |     -     |   2.75   \n",
      "   5    |   620   |   0.177708   |     -      |     -     |   2.77   \n",
      "   5    |   640   |   0.129311   |     -      |     -     |   2.75   \n",
      "   5    |   660   |   0.141218   |     -      |     -     |   2.75   \n",
      "   5    |   680   |   0.124623   |     -      |     -     |   2.75   \n",
      "   5    |   700   |   0.165871   |     -      |     -     |   2.76   \n",
      "   5    |   720   |   0.101163   |     -      |     -     |   2.77   \n",
      "   5    |   740   |   0.126569   |     -      |     -     |   2.76   \n",
      "   5    |   760   |   0.174699   |     -      |     -     |   2.76   \n",
      "   5    |   780   |   0.160066   |     -      |     -     |   2.75   \n",
      "   5    |   788   |   0.118962   |     -      |     -     |   1.10   \n",
      "----------------------------------------------------------------------\n",
      "   5    |    -    |   0.146745   |  0.281483  |   0.74    |  116.22  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=5)\n",
    "train_history = train(bert_classifier, train_dataloader, val_dataloader, epochs=5, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4d7d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(b_ids,  attention_mask, test_type_ids, threshold =0.5):\n",
    "   # speech = speech_file_to_array_fn(path, sampling_rate)\n",
    "   # features = processor(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    #input_values = features.input_values.to(device)\n",
    "    #attention_mask = features.attention_mask.to(device)\n",
    "    b_ids = torch.tensor(b_ids).unsqueeze(0).cuda()\n",
    "    attention_mask = torch.tensor(attention_mask).unsqueeze(0).cuda()\n",
    "    test_type_ids = torch.tensor(test_type_ids).unsqueeze(0).cuda()\n",
    "    #print(input_values.shape)\n",
    "    with torch.no_grad():\n",
    "        logits = bert_classifier(b_ids ,  attention_mask, test_type_ids)\n",
    "        \n",
    "    #preds = logits.argmax(axis = 1)\n",
    "    #accuracy = (preds == labels).cpu().numpy().astype(np.float32)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits).cpu()\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    #print('predictions:',predictions, 'labels:', labels, 'probs', probs)\n",
    "    y_pred = torch.tensor(y_pred)\n",
    "   # outputs = [{\"Emotion\": config.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in enumerate(scores)]\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f38ce252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2104/2104 [00:25<00:00, 81.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "y_pred = []\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    y_pred.append(predict(test_inputs[i], test_masks[i], test_type_ids[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47cf9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.stack(y_pred, axis = 1)\n",
    "y_pred = y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27395448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.77      0.77      0.77       538\n",
      "        fear       0.66      0.62      0.64       201\n",
      "         joy       0.84      0.83      0.83       553\n",
      "     sadness       0.77      0.71      0.74       476\n",
      "    surprise       0.77      0.75      0.76       381\n",
      "\n",
      "   micro avg       0.78      0.75      0.76      2149\n",
      "   macro avg       0.76      0.73      0.75      2149\n",
      "weighted avg       0.78      0.75      0.76      2149\n",
      " samples avg       0.76      0.76      0.76      2149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, y_pred, target_names=label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83861331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x1080 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "f, axes = plt.subplots(1, 5, figsize=(25, 15))\n",
    "axes = axes.ravel()\n",
    "for i in range(5):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix(test_labels[:, i],\n",
    "                                                   y_pred[:, i]),\n",
    "                                  display_labels=[0, i])\n",
    "    disp.plot(ax=axes[i], values_format='.4g')\n",
    "    disp.ax_.set_title(f'class {i}')\n",
    "    if i<10:\n",
    "        disp.ax_.set_xlabel('')\n",
    "    if i%5!=0:\n",
    "        disp.ax_.set_ylabel('')\n",
    "    disp.im_.colorbar.remove()\n",
    "\n",
    "plt.subplots_adjust(wspace=0.10, hspace=0.1)\n",
    "f.colorbar(disp.im_, ax=axes)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
